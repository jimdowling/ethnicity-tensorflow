{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-yemen",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "from tensorflow.contrib.rnn.python.ops.rnn_cell import AttentionCellWrapper\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "\n",
    "def dropout(x, keep_prob):\n",
    "    return tf.nn.dropout(x, keep_prob)\n",
    "\n",
    "\n",
    "def lstm_cell(cell_dim, layer_num, keep_prob):\n",
    "    with tf.variable_scope('LSTM_Cell') as scope:\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(cell_dim, forget_bias=1.0, activation=tf.tanh, state_is_tuple=True)\n",
    "        # cell = AttentionCellWrapper(cell, 10, state_is_tuple=True)\n",
    "        cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "        return tf.contrib.rnn.MultiRNNCell([cell] * layer_num, state_is_tuple=True)\n",
    "\n",
    "\n",
    "def rnn_reshape(inputs, input_dim, max_time_step):\n",
    "    with tf.variable_scope('Reshape') as scope:\n",
    "        \"\"\"\n",
    "        reshape inputs from [batch_size, max_time_step, input_dim] to [max_time_step * (batch_size, input_dim)]\n",
    "\n",
    "        :param inputs: inputs of shape [batch_size, max_time_step, input_dim]\n",
    "        :param input_dim: dimension of input\n",
    "        :param max_time_step: max of time step\n",
    "\n",
    "        :return:\n",
    "            outputs of shape [max_time_step * (batch_size, input_dim)]\n",
    "        \"\"\"\n",
    "        inputs_tr = tf.transpose(inputs, [1, 0, 2])\n",
    "        inputs_tr_reshape = tf.reshape(inputs_tr, [-1, input_dim])\n",
    "        inputs_tr_reshape_split = tf.split(axis=0, num_or_size_splits=max_time_step,\n",
    "                value=inputs_tr_reshape)\n",
    "        return inputs_tr_reshape_split\n",
    "\n",
    "\n",
    "def rnn_model(inputs, input_len, cell, params):\n",
    "    max_time_step = params['max_time_step']\n",
    "    dim_rnn_cell = params['dim_rnn_cell']\n",
    "    with tf.variable_scope('RNN') as scope:\n",
    "        outputs, state = tf.contrib.rnn.static_rnn(cell, inputs, sequence_length=input_len, dtype=tf.float32, scope=scope)\n",
    "        outputs = tf.transpose(tf.stack(outputs), [1, 0, 2])\n",
    "        spread_len = tf.range(0, tf.shape(input_len)[0]) * max_time_step + (input_len - 1)\n",
    "        gathered_outputs = tf.gather(tf.reshape(outputs, [-1, dim_rnn_cell]), spread_len)\n",
    "        return gathered_outputs\n",
    "\n",
    "\n",
    "def bi_rnn_model(inputs, input_len, fw_cell, bw_cell):\n",
    "    with tf.variable_scope('Bi-RNN') as scope:\n",
    "        outputs, _, _ = tf.nn.bidirectional_rnn(fw_cell, bw_cell, inputs,\n",
    "                sequence_length=input_len, dtype=tf.float32, scope=scope)\n",
    "        outputs = tf.transpose(tf.pack(outputs), [1, 0, 2])\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def embedding_lookup(inputs, voca_size, embedding_dim, visual_dir, config, draw=False,\n",
    "        initializer=None, trainable=True, scope='Embedding'):\n",
    "    with tf.variable_scope(scope) as scope:\n",
    "        if initializer is not None:\n",
    "            embedding_table = tf.get_variable(\"embed\",\n",
    "                    initializer=initializer, trainable=trainable, dtype=tf.float32)\n",
    "        else:\n",
    "            embedding_table = tf.get_variable(\"embed\", [voca_size, embedding_dim],\n",
    "                    dtype=tf.float32, trainable=trainable)\n",
    "        inputs_embed = tf.nn.embedding_lookup(embedding_table, inputs)\n",
    "        print(inputs_embed)\n",
    "\n",
    "        if draw:\n",
    "            embedding = config.embeddings.add()\n",
    "            embedding.tensor_name = embedding_table.name\n",
    "            embedding.metadata_path = os.path.join(visual_dir, '%s_metadata.tsv'%scope.name)\n",
    "            return inputs_embed, projector\n",
    "        else:\n",
    "            return inputs_embed, None\n",
    "\n",
    "\n",
    "def mask_by_index(batch_size, input_len, max_time_step):\n",
    "    with tf.variable_scope('Masking') as scope:\n",
    "        input_index = tf.range(0, batch_size) * max_time_step + (input_len - 1)\n",
    "        lengths_transposed = tf.expand_dims(input_index, 1)\n",
    "        lengths_tiled = tf.tile(lengths_transposed, [1, max_time_step])\n",
    "        mask_range = tf.range(0, max_time_step)\n",
    "        range_row = tf.expand_dims(mask_range, 0)\n",
    "        range_tiled = tf.tile(range_row, [batch_size, 1])\n",
    "        mask = tf.less_equal(range_tiled, lengths_tiled)\n",
    "        weight = tf.select(mask, tf.ones([batch_size, max_time_step]),\n",
    "                           tf.zeros([batch_size, max_time_step]))\n",
    "        weight = tf.reshape(weight, [-1])\n",
    "        return weight\n",
    "\n",
    "\n",
    "def linear(inputs, output_dim, dropout_rate=1.0, regularize_rate=0, activation=None, scope='Linear'):\n",
    "    with tf.variable_scope(scope) as scope:\n",
    "        input_dim = inputs.get_shape().as_list()[-1]\n",
    "        inputs = tf.reshape(inputs, [-1, input_dim])\n",
    "        weights = tf.get_variable('Weights', [input_dim, output_dim],\n",
    "                                  initializer=tf.random_normal_initializer())\n",
    "        variable_summaries(weights, scope.name + '/Weights')\n",
    "        biases = tf.get_variable('Biases', [output_dim],\n",
    "                                 initializer=tf.constant_initializer(0.0))\n",
    "        variable_summaries(biases, scope.name + '/Biases')\n",
    "        if activation is None:\n",
    "            return dropout((tf.matmul(inputs, weights) + biases), dropout_rate)\n",
    "        else:\n",
    "            return dropout(activation(tf.matmul(inputs, weights) + biases), dropout_rate)\n",
    "\n",
    "\n",
    "def variable_summaries(var, name):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor.\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean/' + name, mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev/' + name, stddev)\n",
    "        tf.summary.scalar('max/' + name, tf.reduce_max(var))\n",
    "        tf.summary.scalar('min/' + name, tf.reduce_min(var))\n",
    "        tf.summary.histogram(name, var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-spider",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-punishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "from ops import *\n",
    "\n",
    "\n",
    "class RNN(object):\n",
    "    def __init__(self, params, initializer):\n",
    "\n",
    "        # session settings\n",
    "        config = tf.ConfigProto(device_count={'GPU':1})\n",
    "        config.gpu_options.allow_growth = True\n",
    "        config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "        self.session = tf.Session(config=config)\n",
    "        self.params = params\n",
    "        self.model_name = params['model_name']\n",
    "\n",
    "        # hyper parameters\n",
    "        self.learning_rate = params['learning_rate']\n",
    "        self.decay_rate = params['decay_rate']\n",
    "        self.decay_step = params['decay_step']\n",
    "        self.min_grad = params['min_grad']\n",
    "        self.max_grad = params['max_grad']\n",
    "\n",
    "        # rnn parameters\n",
    "        self.max_time_step = params['max_time_step']\n",
    "        self.cell_layer_num = params['lstm_layer']\n",
    "        self.dim_embed_unigram = params['dim_embed_unigram']\n",
    "        self.dim_embed_bigram = params['dim_embed_bigram']\n",
    "        self.dim_embed_trigram = params['dim_embed_trigram']\n",
    "        self.dim_hidden = params['dim_hidden']\n",
    "        self.dim_rnn_cell = params['dim_rnn_cell']\n",
    "        self.dim_unigram = params['dim_unigram'] \n",
    "        self.dim_bigram = params['dim_bigram'] \n",
    "        self.dim_trigram = params['dim_trigram'] \n",
    "        self.dim_output = params['dim_output']\n",
    "        self.ngram = params['ngram']\n",
    "        self.ensemble = params['ensemble']\n",
    "        self.embed = params['embed']\n",
    "        self.embed_trainable = params['embed_trainable']\n",
    "        self.checkpoint_dir = params['checkpoint_dir']\n",
    "        self.initializer = initializer\n",
    "\n",
    "        # input data placeholders\n",
    "        self.unigram = tf.placeholder(tf.int32, [None, self.max_time_step])\n",
    "        self.bigram = tf.placeholder(tf.int32, [None, self.max_time_step])\n",
    "        self.trigram = tf.placeholder(tf.int32, [None, self.max_time_step])\n",
    "        self.lengths = tf.placeholder(tf.int32, [None])\n",
    "        self.labels = tf.placeholder(tf.int32, [None])\n",
    "        self.lstm_dropout = tf.placeholder(tf.float32)\n",
    "        self.hidden_dropout = tf.placeholder(tf.float32)\n",
    "\n",
    "        # model settings\n",
    "        self.global_step = tf.Variable(0, name=\"step\", trainable=False)\n",
    "        self.learning_rate = tf.train.exponential_decay(\n",
    "                self.learning_rate, self.global_step,\n",
    "                self.decay_step, self.decay_rate, staircase=True)\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.optimize = None\n",
    "        self.saver = None\n",
    "        self.losses = None\n",
    "        self.logits = None\n",
    "\n",
    "        # model build\n",
    "        self.merged_summary = None\n",
    "        self.embed_writer = tf.summary.FileWriter(self.checkpoint_dir)\n",
    "        self.embed_config = projector.ProjectorConfig()\n",
    "        self.projector = None\n",
    "        self.build_model()\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "       \n",
    "        # debug initializer\n",
    "        '''\n",
    "        with tf.variable_scope('Unigram', reuse=True):\n",
    "            unigram_embed = tf.get_variable(\"embed\", [self.dim_unigram, self.dim_embed_unigram], dtype=tf.float32)\n",
    "            print(unigram_embed.eval(session=self.session))\n",
    "        '''\n",
    "\n",
    "    def ngram_logits(self, inputs, length, dim_input, dim_embed=None, \n",
    "            initializer=None, trainable=True, scope='ngram'):\n",
    "        with tf.variable_scope(scope) as scope: \n",
    "            fw_cell = lstm_cell(self.dim_rnn_cell, self.cell_layer_num, self.lstm_dropout)\n",
    "            bw_cell = lstm_cell(self.dim_rnn_cell, self.cell_layer_num, self.lstm_dropout)\n",
    "            \n",
    "            if dim_embed is not None:\n",
    "                inputs_embed, self.projector = embedding_lookup(inputs, \n",
    "                        dim_input, dim_embed, self.checkpoint_dir, self.embed_config, \n",
    "                        draw=True, initializer=initializer, trainable=trainable, scope=scope)\n",
    "                inputs_reshape = rnn_reshape(inputs_embed, dim_embed, self.max_time_step)\n",
    "                self.projector.visualize_embeddings(self.embed_writer, self.embed_config)\n",
    "            else:\n",
    "                inputs_reshape = rnn_reshape(tf.one_hot(inputs, dim_input), dim_input, self.max_time_step)\n",
    "            \n",
    "            outputs = rnn_model(inputs_reshape, length, fw_cell, self.params)\n",
    "            return outputs\n",
    "\n",
    "    def build_model(self):\n",
    "        print(\"## Building an RNN model\")\n",
    "\n",
    "        unigram_logits = self.ngram_logits(inputs=self.unigram, \n",
    "                length=self.lengths, \n",
    "                dim_input=self.dim_unigram,\n",
    "                dim_embed=self.dim_embed_unigram if self.embed else None,\n",
    "                initializer=self.initializer[0],\n",
    "                trainable=self.embed_trainable,\n",
    "                scope='Unigram')\n",
    "\n",
    "        bigram_logits = self.ngram_logits(inputs=self.bigram, \n",
    "                length=self.lengths-1, \n",
    "                dim_input=self.dim_bigram,\n",
    "                dim_embed=self.dim_embed_bigram if self.embed else None,\n",
    "                initializer=self.initializer[1],\n",
    "                trainable=self.embed_trainable,\n",
    "                scope='Bigram')\n",
    "        \n",
    "        trigram_logits = self.ngram_logits(inputs=self.trigram, \n",
    "                length=self.lengths-2, \n",
    "                dim_input=self.dim_trigram,\n",
    "                dim_embed=self.dim_embed_trigram if self.embed else None,\n",
    "                initializer=self.initializer[2],\n",
    "                trainable=self.embed_trainable,\n",
    "                scope='Trigram')\n",
    "\n",
    "        if self.ensemble:\n",
    "            total_logits = tf.concat([unigram_logits, bigram_logits, trigram_logits], axis=1)\n",
    "        elif self.ngram == 1:\n",
    "            total_logits = unigram_logits\n",
    "        elif self.ngram == 2:\n",
    "            total_logits = bigram_logits\n",
    "        elif self.ngram == 3:\n",
    "            total_logits = trigram_logits\n",
    "        else:\n",
    "            assert True, 'No specific ngram %d'% ngram\n",
    "\n",
    "        hidden1 = linear(inputs=total_logits, \n",
    "                output_dim=self.dim_hidden,\n",
    "                dropout_rate=self.hidden_dropout,\n",
    "                activation=tf.nn.relu,\n",
    "                scope='Hidden1')\n",
    "        \n",
    "        logits = linear(inputs=total_logits,\n",
    "            output_dim=self.dim_output, \n",
    "            scope='Output')\n",
    "\n",
    "        self.logits = logits \n",
    "        self.losses = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
    "            labels=self.labels))\n",
    "\n",
    "        tf.summary.scalar('Loss', self.losses)\n",
    "        self.variables = tf.trainable_variables()\n",
    "\n",
    "        grads = []\n",
    "        for grad in tf.gradients(self.losses, self.variables):\n",
    "            if grad is not None:\n",
    "                grads.append(tf.clip_by_value(grad, self.min_grad, self.max_grad))\n",
    "            else:\n",
    "                grads.append(grad)\n",
    "        self.optimize = self.optimizer.apply_gradients(zip(grads, self.variables), global_step=self.global_step)\n",
    "\n",
    "        model_vars = [v for v in tf.global_variables()]\n",
    "        print('model variables', [model_var.name for model_var in tf.trainable_variables()])\n",
    "        self.saver = tf.train.Saver(model_vars)\n",
    "        self.merged_summary = tf.summary.merge_all()\n",
    "\n",
    "    @staticmethod\n",
    "    def reset_graph():\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "    def save(self, checkpoint_dir, step):\n",
    "        file_name = \"%s.model\" % self.model_name\n",
    "        self.saver.save(self.session, os.path.join(checkpoint_dir, file_name))\n",
    "        print(\"Model saved\", file_name)\n",
    "\n",
    "    def load(self, checkpoint_dir):\n",
    "        file_name = \"%s.model\" % self.model_name\n",
    "        file_name += \"-10800\"\n",
    "        self.saver.restore(self.session, os.path.join(checkpoint_dir, file_name))\n",
    "        print(\"Model loaded\", file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-temperature",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pprint\n",
    "\n",
    "from time import gmtime, strftime\n",
    "from dataset import get_data, experiment, get_char2vec\n",
    "from model import RNN\n",
    "\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "# Default parameters\n",
    "flags.DEFINE_integer(\"train_epoch\", 3000, \"Epoch to train\")\n",
    "flags.DEFINE_integer(\"dim_unigram\", 82, \"Dimension of input, 42 or 82\")\n",
    "flags.DEFINE_integer(\"dim_bigram\", 1876, \"Dimension of input, 925 or 1876\")\n",
    "flags.DEFINE_integer(\"dim_trigram\", 14767, \"Dimension of input, 8573 or 14767\")\n",
    "flags.DEFINE_integer(\"dim_output\", 127, \"Dimension of output, 95 or 127\")\n",
    "flags.DEFINE_integer(\"max_time_step\", 60, \"Maximum time step of RNN\")\n",
    "flags.DEFINE_integer(\"min_grad\", -5, \"Minimum gradient to clip\")\n",
    "flags.DEFINE_integer(\"max_grad\", 5, \"Maximum gradient to clip\")\n",
    "flags.DEFINE_integer(\"batch_size\", 300, \"Size of batch\")\n",
    "flags.DEFINE_integer(\"ngram\", 3, \"Ngram feature when ensemble = False.\")\n",
    "flags.DEFINE_float(\"decay_rate\", 0.99, \"Decay rate of learning rate\")\n",
    "flags.DEFINE_float(\"decay_step\", 100, \"Decay step of learning rate\")\n",
    "\n",
    "# Validation hyper parameters\n",
    "flags.DEFINE_integer(\"valid_iteration\", 250, \"Number of validation iteration.\")\n",
    "flags.DEFINE_integer(\"dim_rnn_cell\", 200, \"Dimension of RNN cell\")\n",
    "flags.DEFINE_integer(\"dim_rnn_cell_min\", 200, \"Minimum dimension of RNN cell\")\n",
    "flags.DEFINE_integer(\"dim_rnn_cell_max\", 399, \"Maximum dimension of RNN cell\")\n",
    "flags.DEFINE_integer(\"dim_hidden\", 200, \"Dimension of hidden layer\")\n",
    "flags.DEFINE_integer(\"dim_hidden_min\", 200, \"Minimum dimension of hidden layer\")\n",
    "flags.DEFINE_integer(\"dim_hidden_max\", 399, \"Maximum dimension of hidden layer\")\n",
    "flags.DEFINE_integer(\"dim_embed_unigram\", 30, \"Dimension of character embedding\")\n",
    "flags.DEFINE_integer(\"dim_embed_unigram_min\", 10, \"Minimum dimension of character embedding\")\n",
    "flags.DEFINE_integer(\"dim_embed_unigram_max\", 100, \"Maximum dimension of character embedding\")\n",
    "flags.DEFINE_integer(\"dim_embed_bigram\", 100, \"Dimension of character embedding\")\n",
    "flags.DEFINE_integer(\"dim_embed_bigram_min\", 30, \"Minimum dimension of character embedding\")\n",
    "flags.DEFINE_integer(\"dim_embed_bigram_max\", 200, \"Maximum dimension of character embedding\")\n",
    "flags.DEFINE_integer(\"dim_embed_trigram\", 130, \"Dimension of character embedding\")\n",
    "flags.DEFINE_integer(\"dim_embed_trigram_min\", 30, \"Minimum dimension of character embedding\")\n",
    "flags.DEFINE_integer(\"dim_embed_trigram_max\", 320, \"Maximum dimension of character embedding\")\n",
    "flags.DEFINE_integer(\"lstm_layer\", 1, \"Layer number of RNN \")\n",
    "flags.DEFINE_integer(\"lstm_layer_min\", 1, \"Mimimum layer number of RNN \")\n",
    "flags.DEFINE_integer(\"lstm_layer_max\", 1, \"Maximum layer number of RNN \")\n",
    "flags.DEFINE_float(\"lstm_dropout\", 0.5, \"Dropout of RNN cell\")\n",
    "flags.DEFINE_float(\"lstm_dropout_min\", 0.3, \"Minumum dropout of RNN cell\")\n",
    "flags.DEFINE_float(\"lstm_dropout_max\", 0.8, \"Maximum dropout of RNN cell\")\n",
    "flags.DEFINE_float(\"hidden_dropout\", 0.5, \"Dropout rate of hidden layer\")\n",
    "flags.DEFINE_float(\"hidden_dropout_min\", 0.3, \"Minimum dropout rate of hidden layer\")\n",
    "flags.DEFINE_float(\"hidden_dropout_max\", 0.8, \"Maximum dropout rate of hidden layer\")\n",
    "flags.DEFINE_float(\"learning_rate\", 0.01, \"Learning rate of the optimzier\")\n",
    "flags.DEFINE_float(\"learning_rate_min\", 5e-3, \"Minimum learning rate of the optimzier\")\n",
    "flags.DEFINE_float(\"learning_rate_max\", 5e-2, \"Maximum learning rate of the optimzier\")\n",
    "\n",
    "# Model settings\n",
    "flags.DEFINE_boolean(\"default_params\", True, \"True to use default params\")\n",
    "flags.DEFINE_boolean(\"ensemble\", True, \"True to use ensemble ngram\")\n",
    "flags.DEFINE_boolean(\"embed\", True, \"True to use embedding table\")\n",
    "flags.DEFINE_boolean(\"embed_trainable\", False, \"True to use embedding table\")\n",
    "flags.DEFINE_boolean(\"ethnicity\", False, \"True to test on ethnicity\")\n",
    "flags.DEFINE_boolean(\"is_train\", True, \"True for training, False for testing\")\n",
    "flags.DEFINE_boolean(\"is_valid\", True, \"True for validation, False for testing\")\n",
    "flags.DEFINE_boolean(\"continue_train\", False, \"True to continue training from saved checkpoint. False for restarting.\")\n",
    "flags.DEFINE_boolean(\"save\", False, \"True to save\")\n",
    "flags.DEFINE_string(\"model_name\", \"default\", \"Model name, auto saved as YMDHMS\")\n",
    "flags.DEFINE_string(\"checkpoint_dir\", \"./checkpoint/\", \"Directory name to save the checkpoints [checkpoint]\")\n",
    "flags.DEFINE_string(\"data_dir\", \"data/raw\", \"Directory name of input data\")\n",
    "flags.DEFINE_string(\"valid_result_path\", \"result/validation\", \"Validation result save path\")\n",
    "flags.DEFINE_string(\"pred_result_path\", \"result/pred.txt\", \"Prediction result save path\")\n",
    "flags.DEFINE_string(\"detail_result_path\", \"result/detail.txt\", \"Prediction result save path\")\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "\n",
    "def sample_parameters(params):\n",
    "    combination = [\n",
    "            params['dim_hidden'],\n",
    "            params['dim_rnn_cell'],\n",
    "            params['learning_rate'],\n",
    "            params['lstm_dropout'],\n",
    "            params['lstm_layer'],\n",
    "            params['hidden_dropout'],\n",
    "            params['dim_embed_unigram'],\n",
    "            params['dim_embed_bigram'],\n",
    "            params['dim_embed_trigram']\n",
    "    ]\n",
    "\n",
    "    if not params['default_params']:\n",
    "        combination[0] = params['dim_hidden'] = int(np.random.uniform(\n",
    "                params['dim_hidden_min'],\n",
    "                params['dim_hidden_max']) // 50) * 50 \n",
    "        combination[1] = params['dim_rnn_cell'] = int(np.random.uniform(\n",
    "                params['dim_rnn_cell_min'],\n",
    "                params['dim_rnn_cell_max']) // 50) * 50\n",
    "        combination[2] = params['learning_rate'] = float('{0:.5f}'.format(np.random.uniform(\n",
    "                params['learning_rate_min'],\n",
    "                params['learning_rate_max'])))\n",
    "        combination[3] = params['lstm_dropout'] = float('{0:.5f}'.format(np.random.uniform(\n",
    "                params['lstm_dropout_min'],\n",
    "                params['lstm_dropout_max'])))\n",
    "        combination[4] = params['lstm_layer'] = int(np.random.uniform(\n",
    "                params['lstm_layer_min'],\n",
    "                params['lstm_layer_max']))\n",
    "        combination[5] = params['hidden_dropout'] = float('{0:.5f}'.format(np.random.uniform(\n",
    "                params['hidden_dropout_min'],\n",
    "                params['hidden_dropout_max'])))\n",
    "        combination[6] = params['dim_embed_unigram'] = int(np.random.uniform(\n",
    "                params['dim_embed_unigram_min'],\n",
    "                params['dim_embed_unigram_max']) // 10) * 10\n",
    "        combination[7] = params['dim_embed_bigram'] = int(np.random.uniform(\n",
    "                params['dim_embed_bigram_min'],\n",
    "                params['dim_embed_bigram_max']) // 10) * 10\n",
    "        combination[8] = params['dim_embed_trigram'] = int(np.random.uniform(\n",
    "                params['dim_embed_trigram_min'],\n",
    "                params['dim_embed_trigram_max']) // 10) * 10\n",
    "\n",
    "    return params, combination\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    # Save default params and set scope\n",
    "    saved_params = FLAGS.__flags\n",
    "    if saved_params['ensemble']:\n",
    "        model_name = 'ensemble'\n",
    "    elif saved_params['ngram'] == 1:\n",
    "        model_name = 'unigram'\n",
    "    elif saved_params['ngram'] == 2:\n",
    "        model_name = 'bigram'\n",
    "    elif saved_params['ngram'] == 3:\n",
    "        model_name = 'trigram'\n",
    "    else:\n",
    "        assert True, 'Not supported ngram %d'% saved_params['ngram']\n",
    "    model_name += '_embedding' if saved_params['embed'] else '_no_embedding' \n",
    "    saved_params['model_name'] = '%s' % model_name\n",
    "    saved_params['checkpoint_dir'] += model_name\n",
    "    pprint.PrettyPrinter().pprint(saved_params)\n",
    "    saved_dataset = get_data(saved_params) \n",
    "\n",
    "    validation_writer = open(saved_params['valid_result_path'], 'a')\n",
    "    validation_writer.write(model_name + \"\\n\")\n",
    "    validation_writer.write(\"[dim_hidden, dim_rnn_cell, learning_rate, lstm_dropout, lstm_layer, hidden_dropout, dim_embed]\\n\")\n",
    "    validation_writer.write(\"combination\\ttop1\\ttop5\\tepoch\\n\")\n",
    "\n",
    "    # Run the model\n",
    "    for _ in range(saved_params['valid_iteration']):\n",
    "        # Sample parameter sets\n",
    "        params, combination = sample_parameters(saved_params.copy())\n",
    "        dataset = saved_dataset[:]\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        uni_init = get_char2vec(dataset[0][0][:], params['dim_embed_unigram'], dataset[3][0])\n",
    "        bi_init = get_char2vec(dataset[0][1][:], params['dim_embed_bigram'], dataset[3][4])\n",
    "        tri_init = get_char2vec(dataset[0][2][:], params['dim_embed_trigram'], dataset[3][5])\n",
    "        \n",
    "        print(model_name, 'Parameter sets: ', end='')\n",
    "        pprint.PrettyPrinter().pprint(combination)\n",
    "        \n",
    "        rnn_model = RNN(params, [uni_init, bi_init, tri_init])\n",
    "        top1, top5, ep = experiment(rnn_model, dataset, params)\n",
    "        \n",
    "        validation_writer.write(str(combination) + '\\t')\n",
    "        validation_writer.write(str(top1) + '\\t' + str(top5) + '\\tEp:' + str(ep) + '\\n')\n",
    "\n",
    "    validation_writer.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-roots",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "import gensim\n",
    "\n",
    "from random import shuffle\n",
    "from utils import *\n",
    "\n",
    "\n",
    "def get_ethnicity_data(data_dir, params):\n",
    "    is_ethnicity = params['ethnicity']\n",
    "\n",
    "    for root, dir, files in os.walk(data_dir):\n",
    "        unigram_set = []\n",
    "        bigram_set = []\n",
    "        trigram_set = []\n",
    "        length_set = []\n",
    "        labels = []\n",
    "\n",
    "        unigram2idx = {}\n",
    "        idx2unigram = {}\n",
    "        bigram2idx = {}\n",
    "        idx2bigram = {}\n",
    "        trigram2idx = {}\n",
    "        idx2trigram = {}\n",
    "        country2idx = {}\n",
    "        idx2country = {}\n",
    "        country2ethnicity = {}\n",
    "        name_max_len = 0\n",
    "\n",
    "        train_set = []\n",
    "        valid_set = []\n",
    "        test_set = []\n",
    "\n",
    "        for file_cnt, file_name in enumerate(sorted(files)):\n",
    "            data = open(os.path.join(root, file_name))\n",
    "            file_len = 0\n",
    "            \n",
    "            if file_name == '0_unigram_to_idx.txt':\n",
    "                for k, line in enumerate(data):\n",
    "                    file_len = k + 1\n",
    "                    unigram, index = line[:-1].split('\\t')\n",
    "                    unigram2idx[unigram] = int(index)\n",
    "                    idx2unigram[int(index)] = unigram\n",
    "            elif file_name == '1_bigram_to_idx.txt':\n",
    "                for k, line in enumerate(data):\n",
    "                    file_len = k + 1\n",
    "                    bigram, index = line[:-1].split('\\t')\n",
    "                    bigram2idx[bigram] = int(index)\n",
    "                    idx2bigram[int(index)] = bigram\n",
    "            elif file_name == '2_trigram_to_idx.txt':\n",
    "                for k, line in enumerate(data):\n",
    "                    file_len = k + 1\n",
    "                    trigram, index = line[:-1].split('\\t')\n",
    "                    trigram2idx[trigram] = int(index)\n",
    "                    idx2trigram[int(index)] = trigram\n",
    "            elif file_name == 'country_to_idx.txt':\n",
    "                for k, line in enumerate(data):\n",
    "                    file_len = k + 1\n",
    "                    country, index = line[:-1].split('\\t')\n",
    "                    if not is_ethnicity:\n",
    "                        index = k       # Change to index when testing nationality\n",
    "                    country2idx[country] = int(index)\n",
    "                    idx2country[int(index)] = country\n",
    "            elif file_name == 'country_to_ethnicity.txt':\n",
    "                for k, line in enumerate(data):\n",
    "                    file_len = k + 1\n",
    "                    country, eth1, eth2 = line[:-1].split('\\t')\n",
    "                    country2ethnicity[int(country)] = [int(eth1), int(eth2)]\n",
    "            elif 'data_' in file_name:\n",
    "                for k, line in enumerate(data):\n",
    "                    name, nationality = line[:-1].split('\\t')\n",
    "                    name = re.sub(r'\\ufeff', '', name)    # delete BOM\n",
    "\n",
    "                    unigram_vector = [unigram2idx[c] if c in unigram2idx else 0 for c in name]\n",
    "                    bigram_vector= [bigram2idx[c1 + c2] if (c1+c2) in bigram2idx else 0\n",
    "                            for c1, c2 in zip(*[name[i:] for i in range(2)])]\n",
    "                    trigram_vector= [trigram2idx[c1 + c2 + c3] if (c1+c2+c3) in trigram2idx else 0 \n",
    "                            for c1, c2, c3 in zip(*[name[i:] for i in range(3)])]\n",
    "\n",
    "                    # label vector\n",
    "                    nationality = country2idx[nationality]\n",
    "                    if is_ethnicity:\n",
    "                        ethnicity = country2ethnicity[nationality][1]\n",
    "                        if ethnicity < 0:\n",
    "                            continue\n",
    "                    name_length = len(name)\n",
    "\n",
    "                    if name_max_len < len(name):\n",
    "                        name_max_len = len(name)\n",
    "\n",
    "                    unigram_set.append(unigram_vector)\n",
    "                    bigram_set.append(bigram_vector)\n",
    "                    trigram_set.append(trigram_vector)\n",
    "                    length_set.append(name_length)\n",
    "                    if is_ethnicity:\n",
    "                        labels.append(ethnicity)\n",
    "                    else:\n",
    "                        labels.append(nationality)\n",
    "                    file_len = k + 1\n",
    "\n",
    "                if 'train_ch' in file_name:\n",
    "                    train_set = [unigram_set, bigram_set, trigram_set, length_set, labels]\n",
    "                elif 'val' in file_name:\n",
    "                    valid_set = [unigram_set, bigram_set, trigram_set, length_set, labels]\n",
    "                elif 'ijcai' in file_name: # test\n",
    "                    test_set = [unigram_set, bigram_set, trigram_set, length_set, labels]\n",
    "                else:\n",
    "                    assert True, 'not allowed file name %s'% file_name\n",
    "                \n",
    "                unigram_set = []\n",
    "                bigram_set = []\n",
    "                trigram_set = []\n",
    "                length_set = []\n",
    "                labels = []\n",
    "            else:\n",
    "                print('ignoring file', file_name)\n",
    "\n",
    "            print('reading', file_name, 'of length', file_len)\n",
    "\n",
    "    print('total data length:', len(train_set[0]), len(valid_set[0]), len(test_set[0]))\n",
    "    print('shape of data:', np.array(train_set).shape, np.array(valid_set).shape, np.array(test_set).shape)\n",
    "    print('name max length:', name_max_len)\n",
    "\n",
    "    return (train_set, valid_set, test_set,\n",
    "            [idx2unigram, unigram2idx, idx2country, country2ethnicity, idx2bigram, idx2trigram])\n",
    "\n",
    "\n",
    "def get_char2vec(train_set, dim_embed, idx2char):\n",
    "    sentences = []\n",
    "    for sentence in train_set:\n",
    "        char_seq = [idx2char[c] for c in sentence]\n",
    "        sentences.append(char_seq)\n",
    "\n",
    "    model = gensim.models.Word2Vec(sentences, size=dim_embed, window=5, min_count=0, iter=10)\n",
    "    initializer = np.zeros((len(idx2char), dim_embed), dtype=np.float32)\n",
    "\n",
    "    for idx in range(len(idx2char)):\n",
    "        if idx2char[idx] in model:\n",
    "            initializer[idx] = model[idx2char[idx]]\n",
    "   \n",
    "    '''\n",
    "    for alphabet in idx2char.values():\n",
    "        print('most similar to', alphabet, end=' is ')\n",
    "        try:\n",
    "            print(' '.join([(s) for s, _ in model.most_similar(positive=[alphabet], topn=5)]))\n",
    "        except:\n",
    "            print('no values', alphabet)\n",
    "    '''\n",
    "    \n",
    "    return initializer\n",
    "\n",
    "\n",
    "def get_data(params):\n",
    "    ethnicity_dir = params['data_dir']\n",
    "    is_valid = params['is_valid']\n",
    "    train_set, valid_set, test_set, dictionary = get_ethnicity_data(ethnicity_dir, params)\n",
    "\n",
    "    print(train_set[0][0])\n",
    "    print(train_set[1][0])\n",
    "    print(train_set[2][0])\n",
    "    print(train_set[3][0], train_set[4][0])\n",
    "\n",
    "    if not is_valid:\n",
    "        train_set[0] = np.append(train_set[0], valid_set[0], axis=0)\n",
    "        train_set[1] = np.append(train_set[1], valid_set[1], axis=0)\n",
    "        train_set[2] = np.append(train_set[2], valid_set[2], axis=0)\n",
    "        train_set[3] = np.append(train_set[3], valid_set[3], axis=0)\n",
    "        train_set[4] = np.append(train_set[4], valid_set[4], axis=0)\n",
    "    print('shape of data:', np.array(train_set).shape, np.array(valid_set).shape, np.array(test_set).shape)\n",
    "    print('preprocessing done\\n')\n",
    "    \n",
    "    return train_set, valid_set, test_set, dictionary\n",
    "\n",
    "\n",
    "def experiment(model, dataset, params):\n",
    "    print('## Training')\n",
    "    valid_epoch = 1\n",
    "    test_epoch = 1\n",
    "    max_top1 = 0\n",
    "    min_loss = 99999\n",
    "    max_top5 = 0\n",
    "    max_top1_epoch = 0\n",
    "    nochange_cnt = 0\n",
    "    early_stop = 5\n",
    "    checkpoint_dir = params['checkpoint_dir']\n",
    "    continue_train = params['continue_train']\n",
    "    train_epoch = params['train_epoch']\n",
    "    is_save = params['save']\n",
    "    is_valid = params['is_valid']\n",
    "    sess = model.session\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.mkdir(checkpoint_dir)\n",
    "    if continue_train is not False:\n",
    "        model.load(checkpoint_dir)\n",
    "\n",
    "    start_time = time.time()\n",
    "    for epoch_idx in range(train_epoch):\n",
    "        train_cost, train_acc, train_acc5 = run(model, params, dataset[0], is_train=True)\n",
    "        print(\"\\nTraining loss: %.3f, acc1: %.3f, acc5: %.3f, ep: %d\" % (train_cost, train_acc,\n",
    "            train_acc5, epoch_idx))\n",
    "\n",
    "        if (epoch_idx % valid_epoch == 0 or epoch_idx == train_epoch - 1) and is_valid:\n",
    "            valid_cost, valid_acc, valid_acc5 = run(model, params, dataset[1], is_valid=is_valid)\n",
    "            print(\"\\nValidation loss: %.3f, acc1: %.3f, acc5: %.3f, ep: %d\" % (valid_cost, valid_acc,\n",
    "                valid_acc5, epoch_idx))\n",
    "            if valid_acc > max_top1:\n",
    "                max_top1 = valid_acc\n",
    "                max_top5 = valid_acc5\n",
    "                max_top1_epoch = epoch_idx\n",
    "                nochange_cnt = 0\n",
    "            else:\n",
    "                nochange_cnt += 1\n",
    "        elif not is_valid:\n",
    "            if train_cost < min_loss:\n",
    "                min_loss = train_cost\n",
    "                nochange_cnt = 0\n",
    "            else:\n",
    "                nochange_cnt += 1\n",
    "\n",
    "        if epoch_idx % test_epoch == 0 or epoch_idx == train_epoch - 1:\n",
    "            test_cost, test_acc, test_acc5 = run(model, params, dataset[2], dataset[3], is_test=True)\n",
    "            print(\"Testing loss: %.3f, acc1: %.3f, acc5: %.3f\" % (test_cost, test_acc,\n",
    "                test_acc5))\n",
    "            print()\n",
    "            if is_save:\n",
    "                model.save(checkpoint_dir, sess.run(model.global_step))\n",
    "\n",
    "        if nochange_cnt == early_stop:\n",
    "            print(\"Early stopping applied\\n\")\n",
    "            test_cost, test_acc, test_acc5 = run(model, params, dataset[2], dataset[3], is_test=True)\n",
    "            print(\"Testing loss: %.3f, acc1: %.3f, acc5: %.3f\" % (test_cost, test_acc,\n",
    "                test_acc5))\n",
    "            break\n",
    "\n",
    "        # summary = sess.run(model.merged_summary, feed_dict=feed_dict)\n",
    "        # model.train_writer.add_summary(summary, step)\n",
    "\n",
    "    # model.save(checkpoint_dir, sess.run(model.global_step))\n",
    "    model.reset_graph()\n",
    "    return max_top1, max_top5, max_top1_epoch\n",
    "\n",
    "\n",
    "def run(model, params, dataset, dictionary=None, is_train=False, is_valid=False, is_test=False):\n",
    "    batch_size = params['batch_size']\n",
    "    lstm_dropout = params['lstm_dropout']\n",
    "    hidden_dropout = params['hidden_dropout']\n",
    "    output_size = params['dim_output']\n",
    "    max_time_step = params['max_time_step']\n",
    "    sess = model.session\n",
    "    cnt = 0.0\n",
    "    total_cost = 0.0\n",
    "    total_acc = 0.0\n",
    "    total_acc5 = 0.0\n",
    "    total_pred = None\n",
    "    \n",
    "    unigram_set, bigram_set, trigram_set, lengths, labels = dataset\n",
    "    if is_valid or is_test:\n",
    "        lstm_dropout = 1.0\n",
    "        hidden_dropout = 1.0\n",
    "\n",
    "    for datum_idx in range(0, len(unigram_set), batch_size):\n",
    "        batch_unigram = unigram_set[datum_idx:datum_idx+batch_size]\n",
    "        batch_bigram = bigram_set[datum_idx:datum_idx+batch_size]\n",
    "        batch_trigram = trigram_set[datum_idx:datum_idx+batch_size]\n",
    "        batch_lengths= lengths[datum_idx:datum_idx + batch_size]\n",
    "        batch_labels = labels[datum_idx:datum_idx+batch_size]\n",
    "\n",
    "        batch_unigram_onehot = []\n",
    "        batch_bigram_onehot = []\n",
    "        batch_trigram_onehot = []\n",
    "        for unigram in batch_unigram:\n",
    "            unigram_onehot = unigram\n",
    "            while len(unigram_onehot) != max_time_step:\n",
    "                unigram_onehot.append(0)\n",
    "            batch_unigram_onehot.append(unigram_onehot)\n",
    "        for bigram in batch_bigram:\n",
    "            bigram_onehot = bigram\n",
    "            while len(bigram_onehot) != max_time_step:\n",
    "                bigram_onehot.append(0)\n",
    "            batch_bigram_onehot.append(bigram_onehot)\n",
    "        for trigram in batch_trigram:\n",
    "            trigram_onehot = trigram\n",
    "            while len(trigram_onehot) != max_time_step:\n",
    "                trigram_onehot.append(0)\n",
    "            batch_trigram_onehot.append(trigram_onehot)\n",
    "\n",
    "        feed_dict = {model.unigram: batch_unigram_onehot, model.bigram: batch_bigram_onehot,\n",
    "                model.trigram: batch_trigram_onehot, \n",
    "                model.lengths: batch_lengths, model.labels: batch_labels, \n",
    "                model.lstm_dropout: lstm_dropout, model.hidden_dropout: hidden_dropout}\n",
    "        pred, cost, step = sess.run([model.logits, model.losses, model.global_step], feed_dict=feed_dict)\n",
    "\n",
    "        if is_train:\n",
    "            sess.run(model.optimize, feed_dict=feed_dict)\n",
    "        \n",
    "        if (datum_idx % (batch_size*5) == 0) or (datum_idx + batch_size >= len(unigram_set)):\n",
    "            acc = accuracy_score(batch_labels, pred)\n",
    "            acc5 = top_n_acc(batch_labels, pred, 5)\n",
    "            _progress = progress((datum_idx + batch_size) / float(len(unigram_set)))\n",
    "            _progress += \" tr loss: %.3f, acc1: %.3f, acc5: %.3f\" % (cost,\n",
    "                    acc, acc5)\n",
    "            if is_train:\n",
    "                sys.stdout.write(_progress)\n",
    "                sys.stdout.flush()\n",
    "            cnt += 1\n",
    "            total_cost += cost\n",
    "            total_acc += acc\n",
    "            total_acc5 += acc5\n",
    "            \n",
    "        if total_pred is None:\n",
    "            total_pred = pred\n",
    "        else:\n",
    "            total_pred = np.append(total_pred, pred, axis=0)\n",
    "    \n",
    "    is_ethnicity = params['ethnicity']\n",
    "    if is_test and not is_ethnicity:\n",
    "        save_result(total_pred, lengths, labels, unigram_set, dictionary, params['pred_result_path'])\n",
    "    if is_test and is_ethnicity:\n",
    "        save_detail_result(total_pred, labels, lengths, unigram_set, dictionary, params['detail_result_path'])\n",
    "\n",
    "    return total_cost / cnt, total_acc / cnt, total_acc5 / cnt\n",
    "\n",
    "\n",
    "def accuracy_score(labels, logits):\n",
    "    correct_prediction = np.equal(labels, np.argmax(logits, 1))\n",
    "    accuracy = np.mean(correct_prediction.astype(float))\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def top_n_acc(labels, logits, top):\n",
    "    top_n_logits = [logit.argsort()[-top:][::-1] for logit in logits]\n",
    "    correct_prediction = np.array([(pred in topn) for pred, topn in zip(labels, top_n_logits)])\n",
    "    accuracy = np.mean(correct_prediction.astype(float))\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def save_result(logits, indexes, labels, inputs, dictionary, path):\n",
    "    idx2unigram, unigram2idx, idx2country, country2idx, _, _ = dictionary \n",
    "    top_n_logits = [logit.argsort()[-5:][::-1] for logit in logits]\n",
    "\n",
    "    f = open(path, 'w')\n",
    "    for logit, logit_index, label, input in zip(top_n_logits, indexes, labels, inputs):\n",
    "        name = ''.join([idx2unigram[char] for char in input][:logit_index])\n",
    "        pred = 'pred => ' + str(logit[0]) + ':' + idx2country[logit[0]] + '\\n'\n",
    "        pred += 'pred => ' + str(logit[1]) + ':' + idx2country[logit[1]] + '\\n'\n",
    "        pred += 'pred => ' + str(logit[2]) + ':' + idx2country[logit[2]] + '\\n'\n",
    "        pred += 'pred => ' + str(logit[3]) + ':' + idx2country[logit[3]] + '\\n'\n",
    "        pred += 'pred => ' + str(logit[4]) + ':' + idx2country[logit[4]] + '\\n'\n",
    "        corr = 'real => ' + str(label) + ':' + idx2country[label]\n",
    "        result = '[correct]' if logit[0] == label else '[wrong]'\n",
    "        end = '--------------------------------------------'\n",
    "        f.write(result + '\\n' + name + '\\n' + pred + '\\n' + corr + '\\n' + end + '\\n')\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def save_detail_result(logits, labels, indexes, inputs, dictionary, path):\n",
    "    idx2unigram, _, idx2country, country2ethnicity, _, _ = dictionary\n",
    "    tp = dict()\n",
    "    fp = dict()\n",
    "    fn = dict()\n",
    "    tn = dict()\n",
    "\n",
    "    f = open(path, 'w')\n",
    "    for ethnicity in range(13):\n",
    "        key = ethnicity\n",
    "        tp[key] = 0.0\n",
    "        fp[key] = 0.0\n",
    "        fn[key] = 0.0\n",
    "        tn[key] = 0.0\n",
    "        for logit, label in zip(logits, labels):\n",
    "            if np.argmax(logit, 0) == key:\n",
    "                if label == key:\n",
    "                    tp[key] += 1\n",
    "                else:\n",
    "                    fp[key] += 1\n",
    "            else:\n",
    "                if label == key:\n",
    "                    fn[key] += 1\n",
    "                else:\n",
    "                    tn[key] += 1\n",
    "        if tp[key] == 0:\n",
    "            continue\n",
    "        pr = tp[key] / (tp[key] + fp[key])\n",
    "        rc = tp[key] / (tp[key] + fn[key])\n",
    "        f1 = 2*pr*rc / (pr+rc)\n",
    "\n",
    "        f.write(str(ethnicity) + '\\t%.2f\\t%.2f\\t%.2f'% (pr, rc, f1) + '\\n')\n",
    "    f.write('acc %.2f\\n'% ((np.sum(list(tp.values())) + np.sum(list(tn.values()))) \\\n",
    "            / (np.sum(list(tp.values())) + np.sum(list(fp.values())) + np.sum(list(fn.values())) +\n",
    "                np.sum(list(tn.values())))))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-seller",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "from dataset import get_ethnicity_data\n",
    "\n",
    "\n",
    "data_dir = './data/raw'\n",
    "params = {'ethnicity': False}\n",
    "train_set, valid_set, test_set, dictionary = get_ethnicity_data(data_dir, params)\n",
    "vec = 2\n",
    "dic = 5\n",
    "\n",
    "sentences = []\n",
    "for sentence in train_set[vec][:]:\n",
    "    char_seq = [dictionary[dic][c] for c in sentence]\n",
    "    sentences.append(char_seq)\n",
    "for sentence in valid_set[vec][:]:\n",
    "    char_seq = [dictionary[dic][c] for c in sentence]\n",
    "    sentences.append(char_seq)\n",
    "for sentence in test_set[vec][:]:\n",
    "    char_seq = [dictionary[dic][c] for c in sentence]\n",
    "    sentences.append(char_seq)\n",
    "\n",
    "model = gensim.models.Word2Vec(sentences, size=100, window=5, min_count=0, iter=100)\n",
    "\n",
    "for alphabet in dictionary[dic].values():\n",
    "    print('most similar to', alphabet, end=' is ')\n",
    "    try:\n",
    "        print(' '.join([(s) for s, _ in model.most_similar(positive=[alphabet], topn=5)]))\n",
    "    except:\n",
    "        print('no values', alphabet)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
